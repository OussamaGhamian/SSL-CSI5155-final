{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "RS = 42\n",
    "data = pd.read_csv('./bankDataset/bank-full.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
      "4   33       unknown   single    unknown      no        1      no   no   \n",
      "\n",
      "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
      "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
      "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
      "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
      "4  unknown    5   may       198         1     -1         0  unknown  no  \n",
      "       age           job   marital  education default  balance housing loan  \\\n",
      "45206   51    technician   married   tertiary      no      825      no   no   \n",
      "45207   71       retired  divorced    primary      no     1729      no   no   \n",
      "45208   72       retired   married  secondary      no     5715      no   no   \n",
      "45209   57   blue-collar   married  secondary      no      668      no   no   \n",
      "45210   37  entrepreneur   married  secondary      no     2971      no   no   \n",
      "\n",
      "         contact  day month  duration  campaign  pdays  previous poutcome    y  \n",
      "45206   cellular   17   nov       977         3     -1         0  unknown  yes  \n",
      "45207   cellular   17   nov       456         2     -1         0  unknown  yes  \n",
      "45208   cellular   17   nov      1127         5    184         3  success  yes  \n",
      "45209  telephone   17   nov       508         4     -1         0  unknown   no  \n",
      "45210   cellular   17   nov       361         2    188        11    other   no  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   age        45211 non-null  int64 \n",
      " 1   job        45211 non-null  object\n",
      " 2   marital    45211 non-null  object\n",
      " 3   education  45211 non-null  object\n",
      " 4   default    45211 non-null  object\n",
      " 5   balance    45211 non-null  int64 \n",
      " 6   housing    45211 non-null  object\n",
      " 7   loan       45211 non-null  object\n",
      " 8   contact    45211 non-null  object\n",
      " 9   day        45211 non-null  int64 \n",
      " 10  month      45211 non-null  object\n",
      " 11  duration   45211 non-null  int64 \n",
      " 12  campaign   45211 non-null  int64 \n",
      " 13  pdays      45211 non-null  int64 \n",
      " 14  previous   45211 non-null  int64 \n",
      " 15  poutcome   45211 non-null  object\n",
      " 16  y          45211 non-null  object\n",
      "dtypes: int64(7), object(10)\n",
      "memory usage: 5.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age            77\n",
      "job            12\n",
      "marital         3\n",
      "education       4\n",
      "default         2\n",
      "balance      7168\n",
      "housing         2\n",
      "loan            2\n",
      "contact         3\n",
      "day            31\n",
      "month          12\n",
      "duration     1573\n",
      "campaign       48\n",
      "pdays         559\n",
      "previous       41\n",
      "poutcome        4\n",
      "y               2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for missing values:\n",
      " age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "default      0\n",
      "balance      0\n",
      "housing      0\n",
      "loan         0\n",
      "contact      0\n",
      "day          0\n",
      "month        0\n",
      "duration     0\n",
      "campaign     0\n",
      "pdays        0\n",
      "previous     0\n",
      "poutcome     0\n",
      "y            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Check for missing values:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "duplicates = data[data.duplicated()]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates found in the dataset:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicates found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of unknowns in poutcome 0.8174780473778506\n",
      "Precentage of others in poutcome 0.040698060206586895\n"
     ]
    }
   ],
   "source": [
    "poutcome_unknown_count = (data['poutcome'] == 'unknown').sum()\n",
    "poutcome_other_count = (data['poutcome'] == 'other').sum()\n",
    "\n",
    "poutcome_count = len(data['poutcome'])\n",
    "\n",
    "percentage_unknown = poutcome_unknown_count / poutcome_count\n",
    "percentage_other = poutcome_other_count / poutcome_count\n",
    "\n",
    "print(\"Precentage of unknowns in poutcome\", percentage_unknown)\n",
    "print(\"Precentage of others in poutcome\", percentage_other)\n",
    "\n",
    "data = data.drop(columns=['poutcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of unknown per column:\n",
      "\n",
      " age           0.000000\n",
      "job           0.637013\n",
      "marital       0.000000\n",
      "education     4.107407\n",
      "default       0.000000\n",
      "balance       0.000000\n",
      "housing       0.000000\n",
      "loan          0.000000\n",
      "contact      28.798301\n",
      "day           0.000000\n",
      "month         0.000000\n",
      "duration      0.000000\n",
      "campaign      0.000000\n",
      "pdays         0.000000\n",
      "previous      0.000000\n",
      "y             0.000000\n",
      "dtype: float64 \n",
      "\n",
      "Column with the highest precentage of unknown: contact value: 28.798301298356595 \n",
      "\n",
      "Precentage of unknown per row: 0         6.25\n",
      "1         6.25\n",
      "2         6.25\n",
      "3        12.50\n",
      "4        18.75\n",
      "         ...  \n",
      "45206     0.00\n",
      "45207     0.00\n",
      "45208     0.00\n",
      "45209     0.00\n",
      "45210     0.00\n",
      "Length: 45211, dtype: float64 \n",
      "\n",
      "Row with the highest precentage of unknown: 4 value: 18.75 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "unknown_perc_cols = (data == \"unknown\").mean() * 100\n",
    "col_with_highest_perc_id = unknown_perc_cols.idxmax()\n",
    "col_with_highest_perc_val = unknown_perc_cols.max()\n",
    "\n",
    "print(\"Precentage of unknown per column:\\n\\n\", unknown_perc_cols, '\\n')\n",
    "print(\"Column with the highest precentage of unknown:\", col_with_highest_perc_id, 'value:', col_with_highest_perc_val,'\\n')\n",
    "\n",
    "unknown_perc_rows = (data == \"unknown\").mean(axis=1) * 100\n",
    "row_with_highest_perc_id = unknown_perc_rows.idxmax()\n",
    "row_with_highest_perc_val = unknown_perc_rows.max()\n",
    "\n",
    "print(\"Precentage of unknown per row:\", unknown_perc_rows, '\\n')\n",
    "print(\"Row with the highest precentage of unknown:\", row_with_highest_perc_id, 'value:', row_with_highest_perc_val,'\\n')\n",
    "\n",
    "data = data.drop(row_with_highest_perc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = le.fit_transform(data[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.8804011207786462\n",
      "Test Accuracy:  0.8842524329106458\n",
      "Confusion matrix:  [[5493  516]\n",
      " [ 269  504]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "RS = 56\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=RS)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=RS)\n",
    "\n",
    "# SMOTE for handling class imbalance\n",
    "smote = SMOTE(random_state=RS)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Random Forest with hyperparameter tuning\n",
    "rf = RandomForestClassifier(random_state=RS)\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for finding the best hyperparameters and fitting the model\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=2, scoring='accuracy')\n",
    "grid_search.fit(X_res, y_res)\n",
    "\n",
    "# Use the best estimator found\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation and test sets\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation Accuracy: \", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Confusion matrix: \", confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabeled dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(RS)\n",
    "\n",
    "random_unlabeled_points = rng.rand(data['y'].shape[0]) < 0.8\n",
    "unlabel_dataset = data\n",
    "\n",
    "unlabel_dataset['y'][random_unlabeled_points] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-training SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9127101150103214\n",
      "Confusion matrix:  [[5953   56]\n",
      " [ 536  237]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "self_training_model = SelfTrainingClassifier(rf)\n",
    "self_training_model.fit(unlabel_dataset.drop('y', axis=1), unlabel_dataset['y'])\n",
    "\n",
    "y_unlabeled_test_pred = self_training_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_unlabeled_test_pred))\n",
    "print(\"Confusion matrix: \", confusion_matrix(y_test, y_unlabeled_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-training SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf 1 Test Accuracy:  0.8755506607929515\n",
      "clf 1 Confusion matrix:  [[2349   60]\n",
      " [ 279   36]]\n",
      "clf 2 Test Accuracy:  0.9019823788546255\n",
      "clf 2 Confusion matrix:  [[2328   81]\n",
      " [ 186  129]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split the data into labeled and unlabeled sets\n",
    "labeled_data = unlabel_dataset[unlabel_dataset['y'] != -1]\n",
    "unlabeled_data = unlabel_dataset[unlabel_dataset['y'] == -1]\n",
    "\n",
    "# Split labeled data into training and test sets\n",
    "X_labeled = labeled_data.drop('y', axis=1)\n",
    "y_labeled = labeled_data['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.3)\n",
    "\n",
    "# Split features for co-training\n",
    "set_of_features_1 = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "       'loan']\n",
    "set_of_features_2 = ['contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
    "       'previous']\n",
    "\n",
    "X_train_1 = X_train[set_of_features_1]\n",
    "X_train_2 = X_train[set_of_features_2]\n",
    "\n",
    "y_train_1 = y_train\n",
    "y_train_2 = y_train\n",
    "\n",
    "X_test_1 = X_test[set_of_features_1]\n",
    "X_test_2 = X_test[set_of_features_2]\n",
    "\n",
    "# Initialize the classifiers\n",
    "rf_clf1 = RandomForestClassifier()\n",
    "xgb_clf2 = XGBClassifier()\n",
    "\n",
    "# Train both classifiers on the labeled data\n",
    "rf_clf1.fit(X_train_1, y_train)\n",
    "xgb_clf2.fit(X_train_2, y_train)\n",
    "\n",
    "# Co-training loop\n",
    "n_iterations = 10  # Number of iterations\n",
    "n_labels = 10     # Number of samples to label per iteration\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Use rf_clf1 to label data and add to xgb_clf2's training set\n",
    "    if len(unlabeled_data) > 0:\n",
    "        unlabeled_X1 = unlabeled_data[set_of_features_1]\n",
    "        pred_y1 = rf_clf1.predict(unlabeled_X1)\n",
    "        \n",
    "        # Select n_labels instances rf_clf1 is most confident about\n",
    "        pred_proba1 = rf_clf1.predict_proba(unlabeled_X1).max(axis=1)\n",
    "        confident_indices1 = np.argsort(pred_proba1)[-n_labels:]\n",
    "        add_to_train2_X = unlabeled_data.iloc[confident_indices1][set_of_features_2]\n",
    "        add_to_train2_y = pred_y1[confident_indices1]\n",
    "\n",
    "        # Update the training set for xgb_clf2\n",
    "        X_train_2 = pd.concat([X_train_2, add_to_train2_X])\n",
    "        y_train_2 = pd.concat([y_train_2, pd.Series(add_to_train2_y)]) \n",
    "        \n",
    "        # Remove labeled instances from the unlabeled data\n",
    "        unlabeled_data = unlabeled_data.drop(unlabeled_data.index[confident_indices1])\n",
    "\n",
    "        # Retrain xgb_clf2\n",
    "        xgb_clf2.fit(X_train_2, y_train_2)\n",
    "\n",
    "    # Use xgb_clf2 to label data and add to rf_clf1's training set\n",
    "    if len(unlabeled_data) > 0:\n",
    "        unlabeled_X2 = unlabeled_data[set_of_features_2]\n",
    "        pred_y2 = xgb_clf2.predict(unlabeled_X2)\n",
    "        \n",
    "        # Select n_labels instances xgb_clf2 is most confident about\n",
    "        pred_proba2 = xgb_clf2.predict_proba(unlabeled_X2).max(axis=1)\n",
    "        confident_indices2 = np.argsort(pred_proba2)[-n_labels:]\n",
    "        add_to_train1_X = unlabeled_data.iloc[confident_indices2][set_of_features_1]\n",
    "        add_to_train1_y = pred_y2[confident_indices2]\n",
    "\n",
    "        # Update the training set for rf_clf1\n",
    "        X_train_1 = pd.concat([X_train_1, add_to_train1_X])\n",
    "        y_train_1 = pd.concat([y_train_1, pd.Series(add_to_train1_y)])\n",
    "        \n",
    "        # Remove labeled instances from the unlabeled data\n",
    "        unlabeled_data = unlabeled_data.drop(unlabeled_data.index[confident_indices2])\n",
    "\n",
    "        # Retrain rf_clf1\n",
    "        rf_clf1.fit(X_train_1, y_train_1)\n",
    "\n",
    "rf_clf1_pred = rf_clf1.predict(X_test_1)\n",
    "print(\"clf 1 Test Accuracy: \", accuracy_score(y_test, rf_clf1_pred))\n",
    "print(\"clf 1 Confusion matrix: \", confusion_matrix(y_test, rf_clf1_pred))\n",
    "\n",
    "\n",
    "xgb_clf2_pred = xgb_clf2.predict(X_test_2)\n",
    "print(\"clf 2 Test Accuracy: \", accuracy_score(y_test, xgb_clf2_pred))\n",
    "print(\"clf 2 Confusion matrix: \", confusion_matrix(y_test, xgb_clf2_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
