{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "default      0\n",
      "balance      0\n",
      "housing      0\n",
      "loan         0\n",
      "contact      0\n",
      "day          0\n",
      "month        0\n",
      "duration     0\n",
      "campaign     0\n",
      "pdays        0\n",
      "previous     0\n",
      "poutcome     0\n",
      "y            0\n",
      "dtype: int64\n",
      "   age  job  marital  education  default  balance  housing  loan  contact  \\\n",
      "0   58    4        1          2        0     2143        1     0        2   \n",
      "1   44    9        2          1        0       29        1     0        2   \n",
      "2   33    2        1          1        0        2        1     1        2   \n",
      "3   47    1        1          3        0     1506        1     0        2   \n",
      "4   33   11        2          3        0        1        0     0        2   \n",
      "\n",
      "   day  month  duration  campaign  pdays  previous  poutcome  y  \n",
      "0    5      8       261         1     -1         0         3  0  \n",
      "1    5      8       151         1     -1         0         3  0  \n",
      "2    5      8        76         1     -1         0         3  0  \n",
      "3    5      8        92         1     -1         0         3  0  \n",
      "4    5      8       198         1     -1         0         3  0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   age        45211 non-null  int64\n",
      " 1   job        45211 non-null  int64\n",
      " 2   marital    45211 non-null  int64\n",
      " 3   education  45211 non-null  int64\n",
      " 4   default    45211 non-null  int64\n",
      " 5   balance    45211 non-null  int64\n",
      " 6   housing    45211 non-null  int64\n",
      " 7   loan       45211 non-null  int64\n",
      " 8   contact    45211 non-null  int64\n",
      " 9   day        45211 non-null  int64\n",
      " 10  month      45211 non-null  int64\n",
      " 11  duration   45211 non-null  int64\n",
      " 12  campaign   45211 non-null  int64\n",
      " 13  pdays      45211 non-null  int64\n",
      " 14  previous   45211 non-null  int64\n",
      " 15  poutcome   45211 non-null  int64\n",
      " 16  y          45211 non-null  int64\n",
      "dtypes: int64(17)\n",
      "memory usage: 5.9 MB\n",
      "None\n",
      "Test Accuracy:  0.8332350339132999\n",
      "Confusion matrix:  [[9923 2043]\n",
      " [ 219 1379]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "RS = 42\n",
    "data = pd.read_csv('./bankDataset/bank-full.csv', sep=';')\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = LabelEncoder().fit_transform(data[column])\n",
    "\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RS)\n",
    "\n",
    "# Apply undersampling to handle class imbalance\n",
    "under_sampler = RandomUnderSampler(random_state=RS)\n",
    "X_res, y_res = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=RS)\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=2, scoring='accuracy')\n",
    "grid_search.fit(X_res, y_res)\n",
    "\n",
    "\n",
    "# Use the best estimator found\n",
    "best_rf = grid_search.best_estimator_\n",
    "# Predict on the validation and test sets\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Confusion matrix: \", confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45211 45211\n",
      "Test Accuracy:  0.8870539663815984\n",
      "Confusion matrix:  [[11651   315]\n",
      " [ 1217   381]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "\n",
    "rng = np.random.RandomState(RS)\n",
    "\n",
    "random_unlabeled_points = rng.rand(data['y'].shape[0]) < 0.99\n",
    "unlabel_dataset = data\n",
    "\n",
    "unlabel_dataset['y'][random_unlabeled_points] = -1\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "self_training_model = SelfTrainingClassifier(rf)\n",
    "self_training_model.fit(unlabel_dataset.drop('y', axis=1), unlabel_dataset['y'])\n",
    "\n",
    "y_unlabeled_test_pred = self_training_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_unlabeled_test_pred))\n",
    "print(\"Confusion matrix: \", confusion_matrix(y_test, y_unlabeled_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [-1  0  1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     unlabeled_data \u001b[38;5;241m=\u001b[39m unlabeled_data\u001b[38;5;241m.\u001b[39mdrop(add_to_train2\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Retrain clf2\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mclf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Use clf2 to label data and add to clf1's training set\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unlabeled_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:1471\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1468\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m   1470\u001b[0m ):\n\u001b[0;32m-> 1471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1472\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1473\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1474\u001b[0m     )\n\u001b[1;32m   1476\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [-1  0  1]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Split the data into labeled and unlabeled sets\n",
    "labeled_data = unlabel_dataset[unlabel_dataset['y'] != -1]\n",
    "unlabeled_data = unlabel_dataset[unlabel_dataset['y'] == -1]\n",
    "\n",
    "# Split labeled data into training and test sets\n",
    "X_labeled = labeled_data.drop('y', axis=1)\n",
    "y_labeled = labeled_data['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2)\n",
    "\n",
    "# Split features for co-training\n",
    "set_of_features_1 = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "       'loan']\n",
    "set_of_features_2 = ['contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
    "       'previous', 'poutcome']\n",
    "\n",
    "X_train_1 = X_train[set_of_features_1]\n",
    "X_train_2 = X_train[set_of_features_2]\n",
    "X_test_1 = X_test[set_of_features_1]\n",
    "X_test_2 = X_test[set_of_features_2]\n",
    "\n",
    "# Initialize the classifiers\n",
    "clf1 = RandomForestClassifier()\n",
    "clf2 = XGBClassifier()\n",
    "\n",
    "# Train both classifiers on the labeled data\n",
    "clf1.fit(X_train_1, y_train)\n",
    "clf2.fit(X_train_2, y_train)\n",
    "\n",
    "# Co-training loop\n",
    "n_iterations = 10  # Number of iterations\n",
    "n_labels = 10     # Number of samples to label per iteration\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Use clf1 to label data and add to clf2's training set\n",
    "    if len(unlabeled_data) > 0:\n",
    "        unlabeled_X1 = unlabeled_data[set_of_features_1]\n",
    "        pred_y1 = clf1.predict(unlabeled_X1)\n",
    "        \n",
    "        # Select n_labels instances clf1 is most confident about\n",
    "        pred_proba1 = clf1.predict_proba(unlabeled_X1).max(axis=1)\n",
    "        confident_indices1 = np.argsort(pred_proba1)[-n_labels:]\n",
    "        add_to_train2 = unlabeled_data.iloc[confident_indices1]\n",
    "\n",
    "        # Update the training set for clf2\n",
    "        X_train_2 = pd.concat([X_train_2, add_to_train2[set_of_features_2]])\n",
    "        y_train_2 = pd.concat([y_train, add_to_train2['y']])\n",
    "        \n",
    "        # Remove labeled instances from the unlabeled data\n",
    "        unlabeled_data = unlabeled_data.drop(add_to_train2.index)\n",
    "\n",
    "        # Retrain clf2\n",
    "        clf2.fit(X_train_2, y_train_2)\n",
    "\n",
    "    # Use clf2 to label data and add to clf1's training set\n",
    "    if len(unlabeled_data) > 0:\n",
    "        unlabeled_X2 = unlabeled_data[set_of_features_2]\n",
    "        pred_y2 = clf2.predict(unlabeled_X2)\n",
    "        \n",
    "        # Select n_labels instances clf2 is most confident about\n",
    "        pred_proba2 = clf2.predict_proba(unlabeled_X2).max(axis=1)\n",
    "        confident_indices2 = np.argsort(pred_proba2)[-n_labels:]\n",
    "        add_to_train1 = unlabeled_data.iloc[confident_indices2]\n",
    "\n",
    "        # Update the training set for clf1\n",
    "        X_train_1 = pd.concat([X_train_1, add_to_train1[set_of_features_1]])\n",
    "        y_train_1 = pd.concat([y_train, add_to_train1['y']])\n",
    "        \n",
    "        # Remove labeled instances from the unlabeled data\n",
    "        unlabeled_data = unlabeled_data.drop(add_to_train1.index)\n",
    "\n",
    "        # Retrain clf1\n",
    "        clf1.fit(X_train_1, y_train_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
       "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
       "       'previous', 'poutcome', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
